{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADS 509 Assignment 2.1: Tokenization, Normalization, Descriptive Statistics \n",
    "\n",
    "This notebook holds Assignment 2.1 for Module 2 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In the previous assignment you put together Twitter data and lyrics data on two artists. In this assignment we explore some of the textual features of those data sets. If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Blackboard. \n",
    "\n",
    "This assignment asks you to write a short function to calculate some descriptive statistics on a piece of text. Then you are asked to find some interesting and unique statistics on your corpora. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any additional import statements you need here\n",
    "\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I ended up using the M1 Results for Cher and Robyn because my files for Rush and The Killers would not work. I think I\n",
    "# see now from the feedback on my module 1 HW. \n",
    "\n",
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = \"./M1 Results/\"\n",
    "\n",
    "# These subfolders should still work if you correctly stored the \n",
    "# data from the Module 1 assignment\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\"\n",
    "\n",
    "# Section Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "        of unique tokens, lexical diversity, and number of characters. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Fill in the correct values here. I used the len function to get these correct values.\n",
    "    num_tokens = len(tokens)\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    lexical_diversity = num_unique_tokens/num_tokens\n",
    "    num_characters = len(\"\".join(tokens))\n",
    "    \n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    \n",
    "        # print the five most common tokens. I think when I ran this the first time a few of these funcitons were flipepd.\n",
    "        \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            num_characters,\n",
    "            lexical_diversity\n",
    "            ])\n",
    " \n",
    "# Section Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.692 in the data.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"here is some example text with other example text here in this text\"\"\".split()\n",
    "assert(descriptive_stats(text, verbose=True)[0] == 13)\n",
    "assert(descriptive_stats(text, verbose=False)[1] == 9)\n",
    "assert(descriptive_stats(text, verbose=False)[2] == 55)\n",
    "assert(abs(descriptive_stats(text, verbose=False)[3] - 0.69) < 0.02)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Why is it beneficial to use assertion statements in your code? \n",
    "\n",
    "A: To perform a sanity check during the development. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A data frame would work equally well. \n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read lyrics: 313 songs of cher\n",
      "Read lyrics: 93 songs of robyn\n"
     ]
    }
   ],
   "source": [
    "# Read in the lyrics data\n",
    "\n",
    "root_path = \"./M1 Results/lyrics\"\n",
    "lyric_data = dict()\n",
    "for dir_name in os.listdir(root_path):\n",
    "    if dir_name != \".DS_Store\":\n",
    "        file_path = root_path+'/'+dir_name\n",
    "        lyric_data[dir_name] = dict()\n",
    "        for file_name in os.listdir(file_path):\n",
    "            full_file_path = file_path+'/'+file_name\n",
    "            with open(full_file_path, 'r') as f:\n",
    "                text = f.read()\n",
    "                title = text.split(\"\\n\")[0].strip('\"')\n",
    "                artist = dir_name\n",
    "                lyric_data[artist][title] = text\n",
    "        print(f\"Read lyrics: {len(lyric_data[artist].keys())} songs of {dir_name}\")\n",
    "        \n",
    "# Section Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88 Degrees</td>\n",
       "      <td>cher</td>\n",
       "      <td>\"88 Degrees\"\\n\\n\\n\\nStuck in L.A., ain't got n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Different Kind Of Love Song</td>\n",
       "      <td>cher</td>\n",
       "      <td>\"A Different Kind Of Love Song\"\\n\\n\\n\\nWhat if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After All</td>\n",
       "      <td>cher</td>\n",
       "      <td>\"After All\"\\n\\n\\n\\nWell, here we are again\\nI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Again</td>\n",
       "      <td>cher</td>\n",
       "      <td>\"Again\"\\n\\n\\n\\nAgain evening finds me at your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alfie</td>\n",
       "      <td>cher</td>\n",
       "      <td>\"Alfie\"\\n\\n\\n\\nWhat's it all about, Alfie?\\nIs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           title artist  \\\n",
       "0                     88 Degrees   cher   \n",
       "1  A Different Kind Of Love Song   cher   \n",
       "2                      After All   cher   \n",
       "3                          Again   cher   \n",
       "4                          Alfie   cher   \n",
       "\n",
       "                                               lyric  \n",
       "0  \"88 Degrees\"\\n\\n\\n\\nStuck in L.A., ain't got n...  \n",
       "1  \"A Different Kind Of Love Song\"\\n\\n\\n\\nWhat if...  \n",
       "2  \"After All\"\\n\\n\\n\\nWell, here we are again\\nI ...  \n",
       "3  \"Again\"\\n\\n\\n\\nAgain evening finds me at your ...  \n",
       "4  \"Alfie\"\\n\\n\\n\\nWhat's it all about, Alfie?\\nIs...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To make it easier to use further down I read this into a pandas df.\n",
    "\n",
    "lyric_df = (pd.DataFrame(lyric_data)\n",
    "            .reset_index()\n",
    "            .rename(columns = {'index':'title'}))\n",
    "lyric_df.loc[lyric_df['robyn'].isnull()==False,'artist'] = 'robyn'\n",
    "lyric_df.loc[lyric_df['cher'].isnull()==False,'artist'] = 'cher'\n",
    "lyric_df.loc[lyric_df['robyn'].isnull()==False,'lyric'] = lyric_df['robyn']\n",
    "lyric_df.loc[lyric_df['cher'].isnull()==False,'lyric'] = lyric_df['cher']\n",
    "lyric_df.drop(['cher','robyn'],axis = 1, inplace=True)\n",
    "lyric_df.head()\n",
    "\n",
    "# Section Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file:./M1 Results/twitter/cher_followers_data.txt 3994803 rows\n",
      "Read file:./M1 Results/twitter/robynkonichiwa_followers_data.txt 358372 rows\n",
      "Create dataframe: 4353175 rows in total.\n"
     ]
    }
   ],
   "source": [
    "# Read in the twitter data\n",
    "\n",
    "root_path = \"./M1 Results/twitter\"\n",
    "twitter_data = list()\n",
    "for file_name in os.listdir(root_path):\n",
    "    if 'data' in file_name:\n",
    "        file_path = root_path+'/'+file_name\n",
    "        columns = ['screen_name','name','id','location','followers_count','friends_count','description']\n",
    "        df = pd.read_csv(file_path,sep='\\\\t',engine='python')\n",
    "        df['following'] = file_name.split(\"_followers\")[0]\n",
    "        twitter_data.append(df[['following','description']])\n",
    "        print(f\"Read file:{file_path}\",f\"{df.shape[0]} rows\")\n",
    "        \n",
    "twitter_df = pd.concat(twitter_data,axis=0).fillna(\" \")\n",
    "print(f\"Create dataframe: {twitter_df.shape[0]} rows in total.\")\n",
    "\n",
    "# Section Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>following</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cher</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cher</td>\n",
       "      <td>√∞ÔøΩ‚Ñ¢¬ø√∞ÔøΩ≈°‚Ä∫√∞ÔøΩ≈°Àú√∞ÔøΩ≈°≈æ√∞ÔøΩ≈°ÔøΩ √∞ÔøΩ≈°≈ì√∞ÔøΩ≈°≈æ√∞ÔøΩ≈°‚Ñ¢√∞ÔøΩ≈°‚Ñ¢√∞ÔøΩ≈°Àú√∞ÔøΩ≈°‚Ä∫√∞...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cher</td>\n",
       "      <td>163√£≈ΩÔøΩ√Ø¬ºÔøΩ√¶‚Äû‚Ä∫√£ÔøΩ‚Äπ√£ÔøΩ¬£√£ÔøΩ¬∑√∞≈∏‚Äô≈ì26√¶¬≠¬≥√∞≈∏ÔøΩ‚Äô √•¬∑¬•√£‚Ç¨‚Ä°√•¬•¬Ω√£ÔøΩ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cher</td>\n",
       "      <td>csu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cher</td>\n",
       "      <td>Writer @Washinformer @SpelmanCollege alumna #D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  following                                        description\n",
       "0      cher                                                   \n",
       "1      cher  √∞ÔøΩ‚Ñ¢¬ø√∞ÔøΩ≈°‚Ä∫√∞ÔøΩ≈°Àú√∞ÔøΩ≈°≈æ√∞ÔøΩ≈°ÔøΩ √∞ÔøΩ≈°≈ì√∞ÔøΩ≈°≈æ√∞ÔøΩ≈°‚Ñ¢√∞ÔøΩ≈°‚Ñ¢√∞ÔøΩ≈°Àú√∞ÔøΩ≈°‚Ä∫√∞...\n",
       "2      cher  163√£≈ΩÔøΩ√Ø¬ºÔøΩ√¶‚Äû‚Ä∫√£ÔøΩ‚Äπ√£ÔøΩ¬£√£ÔøΩ¬∑√∞≈∏‚Äô≈ì26√¶¬≠¬≥√∞≈∏ÔøΩ‚Äô √•¬∑¬•√£‚Ç¨‚Ä°√•¬•¬Ω√£ÔøΩ...\n",
       "3      cher                                                csu\n",
       "4      cher  Writer @Washinformer @SpelmanCollege alumna #D..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To validate that I created the twitter df with following as the index of which artist the follower is following:\n",
    "\n",
    "twitter_df.head()\n",
    "\n",
    "# Section Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now clean and tokenize your data. Remove punctuation chacters (available in the `punctuation` object in the `string` library), split on whitespace, fold to lowercase, and remove stopwords. Store your cleaned data, which must be accessible as an interable for `descriptive_stats`, in new objects or in new columns in your data frame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'*', ':', '|', ';', '^', '=', '<', '$', '.', \"'\", '/', '+', '[', '#', '`', '!', '\\\\', ',', '>', '?', '@', '%', '(', '&', '{', '}', ')', '_', '\"', '~', ']', '-'}\n"
     ]
    }
   ],
   "source": [
    "punctuation = set(punctuation) # speeds up comparison\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create funcitons to help with the cleaning as asked above:\n",
    "\n",
    "def remove_stop(tokens):\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    return [t for t in tokens if t.lower() not in stopwords]\n",
    "\n",
    "def tokenize(text):\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [t for t in tokens if t.isalpha()]\n",
    "\n",
    "def remove_punctuation(tokens):\n",
    "    return [t for t in tokens if t not in set(punctuation)]\n",
    "\n",
    "def remove_letterlike_symbol(tokens):\n",
    "    \"\"\"\n",
    "    e.g. Japenese, Korean...\n",
    "    \"\"\"\n",
    "    return [t for t in tokens if t.isascii()]\n",
    "\n",
    "pipeline = [str.lower, tokenize, remove_punctuation, remove_stop,remove_letterlike_symbol]\n",
    "def prepare(text, pipeline): \n",
    "    tokens = text\n",
    "    for transform in pipeline: \n",
    "        tokens = transform(tokens)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Section Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                     \n",
       "1                                                     \n",
       "2                                                     \n",
       "3                                                  csu\n",
       "4    writer washinformer spelmancollege alumna dcna...\n",
       "Name: description_token, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create your clean twitter data here\n",
    "\n",
    "twitter_df['description_token']=twitter_df['description'].apply(prepare,pipeline=pipeline)\n",
    "twitter_df['description_token'].head()\n",
    "\n",
    "# Section Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    degrees stuck l got friends hollywood nuts man...\n",
       "1    different kind love song world crazy sane woul...\n",
       "2    well guess must fate tried deep inside known b...\n",
       "3    evening finds door ask could try know quite sa...\n",
       "4    alfie alfie moment live sort alfie meant take ...\n",
       "Name: lyric_token, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create your clean lyrics data here\n",
    "\n",
    "lyric_df['lyric_token']=lyric_df['lyric'].apply(prepare,pipeline=pipeline)\n",
    "lyric_df['lyric_token'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     degrees\n",
       "1    different kind love song\n",
       "2                            \n",
       "3                            \n",
       "4                       alfie\n",
       "Name: title_token, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyric_df['title_token']=lyric_df['title'].apply(prepare,pipeline=pipeline)\n",
    "lyric_df['title_token'].head()\n",
    "\n",
    "# Section Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Descriptive Statistics\n",
    "\n",
    "Call your `descriptive_stats` function on both your lyrics data and your twitter data and for both artists (four total calls). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyrics by Artist: robyn\n",
      "There are 11877 tokens in the data.\n",
      "There are 2053 unique tokens in the data.\n",
      "There are 58057 characters in the data.\n",
      "The lexical diversity is 0.173 in the data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[11877, 2053, 58057, 0.17285509808874294]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calls to descriptive_stats here\n",
    "# I'm starting by breaking this up first for lyrics of Robyn:\n",
    "\n",
    "text = \" \".join(lyric_df[lyric_df['artist']==\"robyn\"]['lyric_token'].tolist()).split()\n",
    "print(\"Lyrics by Artist: robyn\")\n",
    "descriptive_stats(text, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyrics by Artist: cher\n",
      "There are 32601 tokens in the data.\n",
      "There are 3527 unique tokens in the data.\n",
      "There are 158050 characters in the data.\n",
      "The lexical diversity is 0.108 in the data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[32601, 3527, 158050, 0.10818686543357565]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now for cher:\n",
    "\n",
    "text = \" \".join(lyric_df[lyric_df['artist']==\"cher\"]['lyric_token'].tolist()).split()\n",
    "print(\"Lyrics by Artist: cher\")\n",
    "descriptive_stats(text, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter Followers of robynkonichiwa\n",
      "There are 1402208 tokens in the data.\n",
      "There are 155185 unique tokens in the data.\n",
      "There are 8128442 characters in the data.\n",
      "The lexical diversity is 0.111 in the data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1402208, 155185, 8128442, 0.11067188320135102]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now to break down the descriptive stats for followers of Robyn:\n",
    "\n",
    "text = \" \".join(twitter_df[twitter_df['following']==\"robynkonichiwa\"]['description_token'].tolist()).split()\n",
    "print(\"Twitter Followers of robynkonichiwa\")\n",
    "descriptive_stats(text, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter Followers of cher\n",
      "There are 14331010 tokens in the data.\n",
      "There are 617321 unique tokens in the data.\n",
      "There are 82829471 characters in the data.\n",
      "The lexical diversity is 0.043 in the data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[14331010, 617321, 82829471, 0.043075889277866666]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descriptive stats for followers of cher:\n",
    "\n",
    "text = \" \".join(twitter_df[twitter_df['following']==\"cher\"]['description_token'].tolist()).split()\n",
    "print(\"Twitter Followers of cher\")\n",
    "descriptive_stats(text, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: How do you think the \"top 5 words\" would be different if we left stopwords in the data? \n",
    "\n",
    "A: I think the top 5 words would change to include the stop words. I think this would not be useful to include that is why we drop them.\n",
    "\n",
    "---\n",
    "\n",
    "Q: What were your prior beliefs about the lexical diversity between the artists? Does the difference (or lack thereof) in lexical diversity between the artists conform to your prior beliefs? \n",
    "\n",
    "A: I don't know too much about these artists so my initial thought would be that they would have similar lexical diversity. I was wrong because we can see that Roby has a higher lexical diversity compared to cher.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Specialty Statistics\n",
    "\n",
    "The descriptive statistics we have calculated are quite generic. You will now calculate a handful of statistics tailored to these data.\n",
    "\n",
    "1. Ten most common emojis by artist in the twitter descriptions.\n",
    "1. Ten most common hashtags by artist in the twitter descriptions.\n",
    "1. Five most common words in song titles by artist. \n",
    "1. For each artist, a histogram of song lengths (in terms of number of tokens) \n",
    "\n",
    "We can use the `emoji` library to help us identify emojis and you have been given a function to help you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_emoji(s):\n",
    "    return(s in emoji.UNICODE_EMOJI['en'])\n",
    "\n",
    "assert(is_emoji(\"‚ù§Ô∏è\"))\n",
    "assert(not is_emoji(\":-)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emojis üòÅ\n",
    "\n",
    "What are the ten most common emojis by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten most common emojis by robynkonichiwa\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Your code here, first for robyn\n",
    "\n",
    "text = \" \".join(twitter_df[twitter_df['following']==\"robynkonichiwa\"]['description'].tolist()).split()\n",
    "emojis = [x for x in text if is_emoji(x)]\n",
    "counter=collections.Counter(emojis)\n",
    "print(\"Ten most common emojis by robynkonichiwa\")\n",
    "print(counter.most_common(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten most common emojis by cher\n",
      "[('‚Ñ¢', 1)]\n"
     ]
    }
   ],
   "source": [
    "#Emojis for cher:\n",
    "\n",
    "text = \" \".join(twitter_df[twitter_df['following']==\"cher\"]['description'].tolist()).split()\n",
    "emojis = [x for x in text if is_emoji(x)]\n",
    "counter=collections.Counter(emojis)\n",
    "print(\"Ten most common emojis by cher\")\n",
    "print(counter.most_common(10))\n",
    "\n",
    "# Section Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "\n",
    "What are the ten most common hashtags by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten most common hashtags by robynkonichiwa\n",
      "[('#BlackLivesMatter', 311), ('#BLM', 273), ('#blacklivesmatter', 200), ('#1', 187), ('#music', 150), ('#', 150), ('#Music', 93), ('#EDM', 79), ('#blm', 51), ('#TeamFollowBack', 51)]\n"
     ]
    }
   ],
   "source": [
    "# Your code here, first for Robyn:\n",
    "\n",
    "text = \" \".join(twitter_df[twitter_df['following']==\"robynkonichiwa\"]['description'].tolist()).split()\n",
    "hashtags = [x for x in text if x[0]=='#']\n",
    "counter=collections.Counter(hashtags)\n",
    "print(\"Ten most common hashtags by robynkonichiwa\")\n",
    "print(counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten most common hashtags by cher\n",
      "[('#BLM', 7920), ('#Resist', 5007), ('#BlackLivesMatter', 4226), ('#resist', 3134), ('#FBR', 2768), ('#blacklivesmatter', 2464), ('#TheResistance', 2459), ('#1', 2226), ('#', 1965), ('#Resistance', 1518)]\n"
     ]
    }
   ],
   "source": [
    "# Now hashtags for cher:\n",
    "\n",
    "text = \" \".join(twitter_df[twitter_df['following']==\"cher\"]['description'].tolist()).split()\n",
    "hashtags = [x for x in text if x[0]=='#']\n",
    "counter=collections.Counter(hashtags)\n",
    "print(\"Ten most common hashtags by cher\")\n",
    "print(counter.most_common(10))\n",
    "\n",
    "# Section Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song Titles\n",
    "\n",
    "What are the five most common words in song titles by artist? The song titles should be on the first line of the lyrics pages, so if you have kept the raw file contents around, you will not need to re-read the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five most common words by robyn\n",
      "[('love', 5), ('thing', 3), ('girl', 3), ('u', 3), ('like', 2), ('baby', 2), ('music', 2), ('woman', 2), ('girlfriend', 2), ('get', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# After removing stopwords for robyn:\n",
    "\n",
    "text = \" \".join(lyric_df[lyric_df['artist']==\"robyn\"]['title_token'].tolist()).split()\n",
    "counter=collections.Counter(text)\n",
    "print(\"Five most common words by robyn\")\n",
    "print(counter.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five most common words by cher\n",
      "[('love', 36), ('man', 15), ('song', 11), ('come', 7), ('one', 7), ('believe', 6), ('heart', 6), ('time', 6), ('go', 6), ('know', 6)]\n"
     ]
    }
   ],
   "source": [
    "# After removing stopwords for cher:\n",
    "\n",
    "text = \" \".join(lyric_df[lyric_df['artist']==\"cher\"]['title_token'].tolist()).split()\n",
    "counter=collections.Counter(text)\n",
    "print(\"Five most common words by cher\")\n",
    "print(counter.most_common(10))\n",
    "\n",
    "# Section Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song Lengths\n",
    "\n",
    "For each artist, a histogram of song lengths (in terms of number of tokens). If you put the song lengths in a data frame with an artist column, matplotlib will make the plotting quite easy. An example is given to help you out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist\n",
       "Artist 1    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Artist 2    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Name: length, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_replicates = 1000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"artist\" : ['Artist 1'] * num_replicates + ['Artist 2']*num_replicates,\n",
    "    \"length\" : np.concatenate((np.random.poisson(125,num_replicates),np.random.poisson(150,num_replicates)))\n",
    "})\n",
    "\n",
    "df.groupby('artist')['length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the lyrics may be stored with carriage returns or tabs, it may be useful to have a function that can collapse whitespace, using regular expressions, and be used for splitting. \n",
    "\n",
    "Q: What does the regular expression `'\\s+'` match on? \n",
    "\n",
    "A: It will match 1 or more repitions of whitespace with the above regular expression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "collapse_whitespace = re.compile(r'\\s+')\n",
    "\n",
    "def tokenize_lyrics(lyric) : \n",
    "    \"\"\"strip and split on whitespace\"\"\"\n",
    "    return([item.lower() for item in collapse_whitespace.split(lyric)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist\n",
       "cher     AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "robyn    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Name: length, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your lyric length comparison chart here. \n",
    "\n",
    "lyric_df['lyric_split_length'] = lyric_df['lyric'].apply(lambda x: len(tokenize_lyrics(x)))\n",
    "lyric_df.head()\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"artist\" : lyric_df['artist'],\n",
    "    \"length\" : lyric_df['lyric_split_length']\n",
    "})\n",
    "\n",
    "df.groupby('artist')['length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
